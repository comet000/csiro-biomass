{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-4e93493a",
   "metadata": {},
   "source": [
    "# üß™ 02_train_effnet_kaggle ‚Äî CSIRO Image2Biomass (Production Baseline)\n",
    "\n",
    "**Goal:** Train EfficientNet‚ÄëB0 + metadata fusion regressor with portable, identical behavior across macOS (MPS), CPU, and Kaggle GPU ‚Äî within the 9‚Äëhour limit.\n",
    "\n",
    "**Rules:**\n",
    "- Uses only the verified repository context; **no internet**.\n",
    "- Imports from `src/` when present; **does not modify** any `src/` files.\n",
    "- Provides safe in‚Äënotebook fallbacks if a given helper is missing.\n",
    "- Saves artifacts under `output/checkpoints`, `output/logs`, `output/submissions`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91be1526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q albumentations==1.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "code-7e50068a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Project root: /Users/olia_/projects/Kaggle/csiro-biomass\n",
      "üì¶ Using src path: /Users/olia_/projects/Kaggle/csiro-biomass/src\n",
      "üóÇÔ∏è Verified src files:\n",
      "  - config.py: ‚úÖ\n",
      "  - env.py: ‚úÖ\n",
      "  - data_loading.py: ‚úÖ\n",
      "  - data_pipeline.py: ‚úÖ\n",
      "  - model_utils.py: ‚úÖ\n",
      "  - train_utils.py: ‚úÖ\n",
      "  - inference_utils.py: ‚úÖ\n",
      "  - feature_engineering.py: ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# üîß Cell 1 ‚Äî Setup: Paths, Imports, and Environment (kernel-safe)\n",
    "# =============================================================\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# -----------------------------\n",
    "# üîπ Robust PROJECT_ROOT detection\n",
    "# -----------------------------\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if (PROJECT_ROOT / \"src\").exists():\n",
    "    pass\n",
    "elif PROJECT_ROOT.name == \"notebooks\" and (PROJECT_ROOT.parent / \"src\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "else:\n",
    "    PR = PROJECT_ROOT\n",
    "    for _ in range(3):\n",
    "        if (PR / \"src\").exists():\n",
    "            PROJECT_ROOT = PR\n",
    "            break\n",
    "SRC = PROJECT_ROOT / \"src\"\n",
    "\n",
    "# -----------------------------\n",
    "# üîπ Standard input/output paths\n",
    "# -----------------------------\n",
    "INPUT_LOCAL = PROJECT_ROOT / \"input_local\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"output\"\n",
    "CKPT_DIR = OUTPUT_DIR / \"checkpoints\"\n",
    "LOG_DIR = OUTPUT_DIR / \"logs\"\n",
    "SUBM_DIR = OUTPUT_DIR / \"submissions\"\n",
    "\n",
    "for d in [OUTPUT_DIR, CKPT_DIR, LOG_DIR, SUBM_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# üîπ Kernel-safe sys.path patch\n",
    "# Add PROJECT_ROOT to sys.path so that src.* imports work\n",
    "# -----------------------------\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"üìÇ Project root: {PROJECT_ROOT}\")\n",
    "print(f\"üì¶ Using src path: {SRC}\")\n",
    "\n",
    "# -----------------------------\n",
    "# üîπ Verify core src files exist\n",
    "# -----------------------------\n",
    "required_files = [\n",
    "    \"config.py\", \"env.py\", \"data_loading.py\", \"data_pipeline.py\",\n",
    "    \"model_utils.py\", \"train_utils.py\", \"inference_utils.py\", \"feature_engineering.py\"\n",
    "]\n",
    "available = {f: (SRC / f).exists() for f in required_files}\n",
    "print(\"üóÇÔ∏è Verified src files:\")\n",
    "for k, v in available.items():\n",
    "    print(f\"  - {k}: {'‚úÖ' if v else '‚ùå'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "code-57658d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß≠ Device: mps | num_workers=0 | pin_memory=False\n",
      "‚úÖ get_env() from src/env.py\n",
      "üß≠ Using device: mps\n",
      "üë∑ DataLoader workers: 0 | pin_memory: False\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# üåç Cell 2 ‚Äî Device & Worker Configuration (kernel-safe)\n",
    "# =============================================================\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    # Attempt to use src.env get_env()\n",
    "    from src.env import get_env\n",
    "    DEVICE, NUM_WORKERS, PIN_MEMORY = get_env()\n",
    "    print(\"‚úÖ get_env() from src/env.py\")\n",
    "except Exception as e:\n",
    "    # Fallback hardware detection (local deterministic)\n",
    "    if torch.cuda.is_available():\n",
    "        DEVICE = \"cuda\"\n",
    "        NUM_WORKERS = 2\n",
    "        PIN_MEMORY = True\n",
    "    else:\n",
    "        # Force CPU fallback, ignore MPS for clarity\n",
    "        DEVICE = \"cpu\"\n",
    "        NUM_WORKERS = 0\n",
    "        PIN_MEMORY = False\n",
    "    print(f\"‚ö†Ô∏è Fallback env applied: DEVICE={DEVICE}, NUM_WORKERS={NUM_WORKERS}, PIN_MEMORY={PIN_MEMORY} | reason: {e}\")\n",
    "\n",
    "print(f\"üß≠ Using device: {DEVICE}\")\n",
    "print(f\"üë∑ DataLoader workers: {NUM_WORKERS} | pin_memory: {PIN_MEMORY}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "code-2e9f3aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Seeds set for deterministic runs on mps\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# üé≤ Cell 3 ‚Äî Reproducibility: Deterministic Seeds\n",
    "# =============================================================\n",
    "SEED = 42\n",
    "\n",
    "# -----------------------------\n",
    "# üîπ Python & NumPy seeds\n",
    "# -----------------------------\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# üîπ Device-specific seeds\n",
    "# -----------------------------\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "elif DEVICE == \"mps\":\n",
    "    # MPS backend seed handling (if needed)\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# üîπ Deterministic backend behavior\n",
    "# -----------------------------\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"‚úÖ Seeds set for deterministic runs on {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e7ce57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loaded train.csv ‚Üí shape: (1785, 9)\n",
      "‚úÖ Pivoted dataset shape: (357, 6)\n",
      "üíæ Saved pivoted training data ‚Üí ../input_local/train_pivoted.csv\n",
      "üßÆ Unique images before pivot: 357\n",
      "‚úÖ Pivot complete. Ready for image-only multi-output training.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# üß© Pivot train.csv for multi-output image regression (../input_local/)\n",
    "# =============================================================\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Correct simple path ---\n",
    "train_path = Path(\"../input_local/train.csv\")  # üëà one level up from notebooks\n",
    "\n",
    "if not train_path.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Could not find {train_path.resolve()}\")\n",
    "\n",
    "# --- Load original training data ---\n",
    "df = pd.read_csv(train_path)\n",
    "print(f\"üìÇ Loaded train.csv ‚Üí shape: {df.shape}\")\n",
    "\n",
    "# --- Pivot so each image_path appears once with 5 target columns ---\n",
    "pivot_df = (\n",
    "    df.pivot_table(\n",
    "        index=\"image_path\",\n",
    "        columns=\"target_name\",\n",
    "        values=\"target\",\n",
    "        aggfunc=\"first\"\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# --- Drop incomplete rows if any ---\n",
    "pivot_df = pivot_df.dropna(subset=[\"Dry_Total_g\"])\n",
    "print(f\"‚úÖ Pivoted dataset shape: {pivot_df.shape}\")\n",
    "\n",
    "# --- Save pivoted file beside train.csv ---\n",
    "pivot_path = train_path.parent / \"train_pivoted.csv\"\n",
    "pivot_df.to_csv(pivot_path, index=False)\n",
    "print(f\"üíæ Saved pivoted training data ‚Üí {pivot_path}\")\n",
    "\n",
    "# --- Sanity checks ---\n",
    "unique_images = df[\"image_path\"].nunique()\n",
    "print(f\"üßÆ Unique images before pivot: {unique_images}\")\n",
    "assert unique_images == len(pivot_df), \"Pivot mismatch ‚Äî check for missing targets\"\n",
    "\n",
    "print(\"‚úÖ Pivot complete. Ready for image-only multi-output training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "code-0f1dcd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using Albumentations for augmentations.\n",
      "‚úÖ Train dataset: 357 samples | Test dataset: 5 samples\n",
      "üéûÔ∏è Batch shapes ‚Äî images: torch.Size([8, 3, 518, 518]), targets: torch.Size([8, 5])\n",
      "‚úÖ Dataloader sanity check passed (IMG_SIZE=518, backbone=DINOv2 ViT-B/14).\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# üì¶ Cell 4 ‚Äî Build Datasets and DataLoaders (DINOv2-Compatible, 518√ó518)\n",
    "# =============================================================\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# --- Try Albumentations; fallback to torchvision if unavailable ---\n",
    "try:\n",
    "    import albumentations as A\n",
    "    from albumentations.pytorch import ToTensorV2\n",
    "    USE_ALB = True\n",
    "    print(\"‚úÖ Using Albumentations for augmentations.\")\n",
    "except ImportError:\n",
    "    from torchvision import transforms\n",
    "    USE_ALB = False\n",
    "    print(\"‚ö†Ô∏è Albumentations not found ‚Äî falling back to torchvision transforms.\")\n",
    "\n",
    "# -----------------------------\n",
    "# üîπ Paths\n",
    "# -----------------------------\n",
    "DATA_ROOT = Path(\"../input_local\")\n",
    "TRAIN_CSV = DATA_ROOT / \"train_pivoted.csv\"\n",
    "TEST_CSV  = DATA_ROOT / \"test.csv\"\n",
    "\n",
    "if not TRAIN_CSV.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Missing: {TRAIN_CSV.resolve()}\")\n",
    "if not TEST_CSV.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Missing: {TEST_CSV.resolve()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# üîπ Global constants (linked to model)\n",
    "# -----------------------------\n",
    "IMG_SIZE = globals().get(\"IMG_SIZE\", 518)  # ‚úÖ match DINOv2 ViT-B/14\n",
    "USE_LOG_TARGET = True\n",
    "LOG_EPS = 1.0\n",
    "BATCH_SIZE = 8 if DEVICE == \"mps\" else (32 if DEVICE == \"cuda\" else 8)  # DINOv2 = larger images\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# -----------------------------\n",
    "# üîπ Define augmentations (stronger for ViTs)\n",
    "# -----------------------------\n",
    "if USE_ALB:\n",
    "    train_tfms = A.Compose([\n",
    "        A.Resize(IMG_SIZE, IMG_SIZE, interpolation=1),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.15, rotate_limit=30, p=0.7),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.4),\n",
    "        A.HueSaturationValue(p=0.3),\n",
    "        A.RGBShift(p=0.3),\n",
    "        A.RandomGamma(p=0.3),\n",
    "        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    valid_tfms = A.Compose([\n",
    "        A.Resize(IMG_SIZE, IMG_SIZE, interpolation=1),\n",
    "        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "else:\n",
    "    from torchvision import transforms\n",
    "    train_tfms = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                             std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    valid_tfms = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                             std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "# -----------------------------\n",
    "# üîπ Dataset class\n",
    "# -----------------------------\n",
    "class ImageOnlyDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_root, transform, train=True, use_log=False):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.img_root = Path(img_root)\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.use_log = use_log\n",
    "        if train:\n",
    "            self.target_cols = [\"Dry_Clover_g\", \"Dry_Dead_g\", \"Dry_Green_g\", \"Dry_Total_g\", \"GDM_g\"]\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = self.img_root / row[\"image_path\"]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if USE_ALB:\n",
    "            image = np.array(image)\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        else:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.train:\n",
    "            target = row[self.target_cols].values.astype(\"float32\")\n",
    "            if self.use_log:\n",
    "                target = np.log1p(target)\n",
    "            return image, torch.tensor(target, dtype=torch.float32)\n",
    "        else:\n",
    "            sample_id = row.get(\"sample_id\", f\"test_{idx}\")\n",
    "            return image, sample_id\n",
    "\n",
    "# -----------------------------\n",
    "# üîπ DataLoaders\n",
    "# -----------------------------\n",
    "train_ds = ImageOnlyDataset(TRAIN_CSV, img_root=DATA_ROOT, transform=train_tfms, train=True, use_log=USE_LOG_TARGET)\n",
    "test_ds  = ImageOnlyDataset(TEST_CSV,  img_root=DATA_ROOT, transform=valid_tfms, train=False)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=False)\n",
    "test_dl  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=False)\n",
    "\n",
    "print(f\"‚úÖ Train dataset: {len(train_ds)} samples | Test dataset: {len(test_ds)} samples\")\n",
    "imgs, targets = next(iter(train_dl))\n",
    "print(f\"üéûÔ∏è Batch shapes ‚Äî images: {imgs.shape}, targets: {targets.shape}\")\n",
    "print(f\"‚úÖ Dataloader sanity check passed (IMG_SIZE={IMG_SIZE}, backbone=DINOv2 ViT-B/14).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "code-c198cec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/vit_base_patch14_dinov2.lvd142m)\n",
      "INFO:timm.models._hub:[timm/vit_base_patch14_dinov2.lvd142m] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded backbone: vit_base_patch14_dinov2.lvd142m\n",
      "üßÆ Trainable parameters: 87.17 M | Device: mps\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# üß† Cell 5 ‚Äî Multi-Output Model Definition (DINOv2 ViT-B/14, Stable)\n",
    "# =============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# üîπ DINOv2 model family + image sizes\n",
    "# -------------------------------------------------------------\n",
    "IMG_SIZE_MAP = {\n",
    "    \"vit_base_patch14_dinov2.lvd142m\": 518,  # true DINOv2 ViT-B/14\n",
    "    \"efficientnet_b3\": 300,\n",
    "}\n",
    "NUM_OUTPUTS = 5\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# üîπ Model wrapper\n",
    "# -------------------------------------------------------------\n",
    "class ImageRegressor(nn.Module):\n",
    "    def __init__(self, backbone_name=\"vit_base_patch14_dinov2.lvd142m\", pretrained=True, num_outputs=NUM_OUTPUTS, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.backbone_name = backbone_name\n",
    "        try:\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=pretrained, num_classes=0)\n",
    "            print(f\"‚úÖ Loaded backbone: {backbone_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not load {backbone_name} ({e}); using EfficientNet-B3 fallback.\")\n",
    "            self.backbone_name = \"efficientnet_b3\"\n",
    "            self.backbone = timm.create_model(\"efficientnet_b3\", pretrained=True, num_classes=0)\n",
    "\n",
    "        # DINOv2 uses embed_dim, EfficientNet uses num_features\n",
    "        in_features = getattr(self.backbone, \"embed_dim\", None) or getattr(self.backbone, \"num_features\", 1024)\n",
    "        hidden = 768 if \"dinov2\" in self.backbone_name else 512\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, num_outputs),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        return self.head(feats)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# üîπ Initialize\n",
    "# -------------------------------------------------------------\n",
    "BACKBONE = \"vit_base_patch14_dinov2.lvd142m\"\n",
    "IMG_SIZE = IMG_SIZE_MAP.get(BACKBONE, 300)\n",
    "model = ImageRegressor(backbone_name=BACKBONE, pretrained=True, num_outputs=NUM_OUTPUTS).to(DEVICE)\n",
    "globals()[\"IMG_SIZE\"] = IMG_SIZE\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"üßÆ Trainable parameters: {n_params/1e6:.2f} M | Device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "code-643823c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ vit_base_patch14_dinov2.lvd142m | 120 epochs | IMG_SIZE=518 | Device=mps\n",
      "Samples = 357 | Batch = 8\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab022fdf8c048e1838add374be8de97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "‚è≥ Training Progress:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/120 | RMSE 35.62 | MAE 24.57 | R¬≤ -0.903 | 85.8s | ETA 170.3 min\n",
      "Epoch 010/120 | RMSE 23.61 | MAE 14.40 | R¬≤ 0.163 | 83.0s | ETA 155.9 min\n",
      "üîì Unfroze top 30 % of ViT layers at epoch 20\n",
      "Epoch 020/120 | RMSE 18.53 | MAE 11.08 | R¬≤ 0.485 | 84.3s | ETA 141.2 min\n",
      "Epoch 030/120 | RMSE 14.68 | MAE 8.19 | R¬≤ 0.677 | 125.3s | ETA 151.5 min\n",
      "Epoch 040/120 | RMSE 13.23 | MAE 7.61 | R¬≤ 0.738 | 131.8s | ETA 144.2 min\n",
      "Epoch 050/120 | RMSE 11.87 | MAE 6.65 | R¬≤ 0.789 | 140.0s | ETA 132.4 min\n",
      "Epoch 060/120 | RMSE 10.80 | MAE 5.88 | R¬≤ 0.825 | 137.5s | ETA 117.1 min\n",
      "Epoch 070/120 | RMSE 10.05 | MAE 5.57 | R¬≤ 0.849 | 127.6s | ETA 99.2 min\n",
      "Epoch 080/120 | RMSE 11.13 | MAE 6.14 | R¬≤ 0.814 | 128.1s | ETA 80.2 min\n",
      "Epoch 090/120 | RMSE 11.96 | MAE 6.17 | R¬≤ 0.785 | 133.0s | ETA 60.8 min\n",
      "Epoch 100/120 | RMSE 11.10 | MAE 6.01 | R¬≤ 0.815 | 133.3s | ETA 40.7 min\n",
      "Epoch 110/120 | RMSE 9.55 | MAE 5.08 | R¬≤ 0.863 | 127.6s | ETA 20.5 min\n",
      "Epoch 120/120 | RMSE 9.84 | MAE 4.91 | R¬≤ 0.855 | 134.1s | ETA 0.0 min\n",
      "\n",
      "‚úÖ Best checkpoint loaded | RMSE 8.99\n",
      "üèÅ Total training time 247.9 min | Backbone vit_base_patch14_dinov2.lvd142m\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# üèãÔ∏è Cell 6 ‚Äî 30-Minute Quiet Training (DINOv2 ViT-B/14, Static ETA Bar)\n",
    "# =============================================================\n",
    "import os, time, logging, warnings, numpy as np, torch, torch.nn as nn\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from src.config import PROJECT_ROOT\n",
    "\n",
    "# -------------------------------\n",
    "# üîï Silence framework noise\n",
    "# -------------------------------\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "logging.getLogger(\"torch\").setLevel(logging.ERROR)\n",
    "\n",
    "# -------------------------------\n",
    "# üîß Config\n",
    "# -------------------------------\n",
    "USE_AMP = (DEVICE == \"cuda\")\n",
    "EPOCHS = 120               # ‚âà 30 min on MPS / CPU\n",
    "LR = 2e-5                  # small LR for ViT fine-tuning\n",
    "WD = 1e-4\n",
    "WARMUP_EPOCHS = 5\n",
    "UNFREEZE_AT = 20\n",
    "PATIENCE = None            # üö´ disable early-stop completely\n",
    "PRINT_EVERY = 10\n",
    "\n",
    "BACKBONE_NAME = globals().get(\"BACKBONE\", \"vit_base_patch14_dinov2.lvd142m\")\n",
    "BEST_CKPT = PROJECT_ROOT / \"output\" / \"checkpoints\" / f\"{BACKBONE_NAME}_best.pth\"\n",
    "BEST_CKPT.parent.mkdir(parents=True, exist_ok=True)\n",
    "BEST_CKPT = str(BEST_CKPT)\n",
    "\n",
    "# -------------------------------\n",
    "# üîπ Metrics\n",
    "# -------------------------------\n",
    "def _inverse_log(x): return np.expm1(x)\n",
    "\n",
    "def _compute_metrics(y_true, y_pred, use_log=True):\n",
    "    if use_log:\n",
    "        y_true, y_pred = _inverse_log(y_true), _inverse_log(y_pred)\n",
    "    y_true, y_pred = y_true.flatten(), y_pred.flatten()\n",
    "    rmse = float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
    "    mae  = float(np.mean(np.abs(y_true - y_pred)))\n",
    "    r2   = float(1 - np.sum((y_true - y_pred)**2) /\n",
    "                 (np.sum((y_true - np.mean(y_true))**2) + 1e-12))\n",
    "    return dict(rmse=rmse, mae=mae, r2=r2)\n",
    "\n",
    "# -------------------------------\n",
    "# üîπ Optimizer & Scheduler\n",
    "# -------------------------------\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "criterion = nn.SmoothL1Loss(beta=0.5)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "def adjust_lr(epoch):\n",
    "    if epoch <= WARMUP_EPOCHS:\n",
    "        lr = LR * (epoch / WARMUP_EPOCHS)\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = lr\n",
    "\n",
    "# -------------------------------\n",
    "# üîπ Freeze / Unfreeze\n",
    "# -------------------------------\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = False  # start frozen\n",
    "\n",
    "def unfreeze_backbone_layers(model, ratio=0.3):\n",
    "    layers = list(model.backbone.parameters())\n",
    "    cutoff = int(len(layers) * (1 - ratio))\n",
    "    for p in layers[cutoff:]:\n",
    "        p.requires_grad = True\n",
    "\n",
    "# -------------------------------\n",
    "# üîπ Epoch Routine\n",
    "# -------------------------------\n",
    "def run_epoch(dl, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    losses, y_true_all, y_pred_all = [], [], []\n",
    "    for imgs, targets in dl:\n",
    "        imgs, targets = imgs.to(DEVICE), targets.to(DEVICE)\n",
    "        with torch.set_grad_enabled(train):\n",
    "            with torch.amp.autocast(device_type=\"cuda\" if USE_AMP else \"cpu\", enabled=USE_AMP):\n",
    "                preds = model(imgs)\n",
    "                loss = criterion(preds, targets)\n",
    "            if train:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "        losses.append(loss.item())\n",
    "        y_true_all.append(targets.cpu().numpy())\n",
    "        y_pred_all.append(preds.detach().cpu().numpy())\n",
    "    y_true, y_pred = np.concatenate(y_true_all), np.concatenate(y_pred_all)\n",
    "    return np.mean(losses), _compute_metrics(y_true, y_pred, use_log=USE_LOG_TARGET)\n",
    "\n",
    "# -------------------------------\n",
    "# üîπ Main Training Loop (quiet)\n",
    "# -------------------------------\n",
    "best_rmse, best_state = float(\"inf\"), None\n",
    "epoch_times = []\n",
    "\n",
    "print(f\"üöÄ {BACKBONE_NAME} | {EPOCHS} epochs | IMG_SIZE={IMG_SIZE} | Device={DEVICE}\")\n",
    "print(f\"Samples = {len(train_ds)} | Batch = {train_dl.batch_size}\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "progress = tqdm(total=EPOCHS, desc=\"‚è≥ Training Progress\", position=0, leave=True, miniters=1)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    adjust_lr(epoch)\n",
    "    start = time.time()\n",
    "    tr_loss, tr_metrics = run_epoch(train_dl, train=True)\n",
    "    scheduler.step()\n",
    "    epoch_time = time.time() - start\n",
    "    epoch_times.append(epoch_time)\n",
    "    progress.update(1)\n",
    "    eta = (EPOCHS - epoch) * np.mean(epoch_times) / 60\n",
    "\n",
    "    if epoch == UNFREEZE_AT:\n",
    "        unfreeze_backbone_layers(model, ratio=0.3)\n",
    "        print(f\"üîì Unfroze top 30 % of ViT layers at epoch {epoch}\")\n",
    "\n",
    "    if (epoch % PRINT_EVERY == 0) or (epoch in {1, UNFREEZE_AT, EPOCHS}):\n",
    "        print(f\"Epoch {epoch:03d}/{EPOCHS} | RMSE {tr_metrics['rmse']:.2f} | \"\n",
    "              f\"MAE {tr_metrics['mae']:.2f} | R¬≤ {tr_metrics['r2']:.3f} | \"\n",
    "              f\"{epoch_time:.1f}s | ETA {eta:.1f} min\")\n",
    "\n",
    "    if tr_metrics[\"rmse\"] < best_rmse - 1e-5:\n",
    "        best_rmse = tr_metrics[\"rmse\"]\n",
    "        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        torch.save(best_state, BEST_CKPT)\n",
    "\n",
    "progress.close()\n",
    "total_time = (time.time() - t0) / 60\n",
    "\n",
    "# -------------------------------\n",
    "# üîπ Finalize\n",
    "# -------------------------------\n",
    "if best_state:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(f\"\\n‚úÖ Best checkpoint loaded | RMSE {best_rmse:.2f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No improvement ‚Äî using final weights.\")\n",
    "print(f\"üèÅ Total training time {total_time:.1f} min | Backbone {BACKBONE_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7fdc08",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "beb6b6ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "149565fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "code-37a514f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loaded test.csv ‚Üí shape: (5, 3)\n",
      "‚úÖ Inference complete ‚Üí predictions shape: (5, 5)\n",
      "üïí Inference time: 19.6s\n",
      "‚ö†Ô∏è Adjusting predictions: 5 rows vs 1 images\n",
      "‚úÖ Saved submission ‚Üí /Users/olia_/projects/Kaggle/csiro-biomass/output/submissions/submission.csv\n",
      "üì¶ Submission shape: (5, 4)\n",
      "count     5.000000\n",
      "mean     26.450842\n",
      "std      18.799717\n",
      "min       0.005757\n",
      "25%      20.874638\n",
      "50%      27.795448\n",
      "75%      31.519236\n",
      "max      52.059128\n",
      "üèÅ Ready for Kaggle upload.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# üìà Cell 7 ‚Äî Final Inference & Kaggle Submission (DINOv2 + Extended TTA)\n",
    "# =============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# -------------------------------\n",
    "# üîß Device & model prep\n",
    "# -------------------------------\n",
    "device = next(model.parameters()).device\n",
    "model.eval()\n",
    "\n",
    "# -------------------------------\n",
    "# üìÇ Paths\n",
    "# -------------------------------\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"input_local\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"output\" / \"submissions\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "test_path = DATA_DIR / \"test.csv\"\n",
    "img_root = DATA_DIR\n",
    "if not test_path.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Missing {test_path.resolve()}\")\n",
    "\n",
    "# -------------------------------\n",
    "# üìë Load test.csv\n",
    "# -------------------------------\n",
    "test_df = pd.read_csv(test_path)\n",
    "print(f\"üìÇ Loaded test.csv ‚Üí shape: {test_df.shape}\")\n",
    "\n",
    "# -------------------------------\n",
    "# üß† Build test dataset\n",
    "# -------------------------------\n",
    "IMG_SIZE = getattr(train_ds, \"img_size\", 518)\n",
    "test_ds = ImageOnlyDataset(\n",
    "    csv_path=test_path,\n",
    "    img_root=img_root,\n",
    "    transform=valid_tfms,\n",
    "    train=False\n",
    ")\n",
    "test_dl = DataLoader(test_ds, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "# =============================================================\n",
    "# üîπ Inference with Extended TTA (flips + 90¬∞ rotations)\n",
    "# =============================================================\n",
    "def infer_with_tta(imgs):\n",
    "    \"\"\"\n",
    "    Perform inference with flips and 90¬∞ rotations.\n",
    "    DINOv2 ViTs are rotation-tolerant, so averaging these helps generalize.\n",
    "    \"\"\"\n",
    "    variants = [\n",
    "        imgs,\n",
    "        torch.flip(imgs, dims=[2]),              # vertical\n",
    "        torch.flip(imgs, dims=[3]),              # horizontal\n",
    "        torch.rot90(imgs, 1, [2, 3]),\n",
    "        torch.rot90(imgs, 2, [2, 3]),\n",
    "        torch.rot90(imgs, 3, [2, 3]),\n",
    "    ]\n",
    "    preds_all = [model(v) for v in variants]\n",
    "    return torch.stack(preds_all).mean(dim=0)\n",
    "\n",
    "# =============================================================\n",
    "# üîπ Run Inference Loop\n",
    "# =============================================================\n",
    "all_preds = []\n",
    "start = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, ids in test_dl:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        preds = infer_with_tta(imgs)\n",
    "        preds = preds.cpu().numpy()\n",
    "        if USE_LOG_TARGET:\n",
    "            preds = np.expm1(preds)  # inverse log transform\n",
    "        all_preds.append(preds)\n",
    "\n",
    "preds = np.concatenate(all_preds, axis=0)\n",
    "print(f\"‚úÖ Inference complete ‚Üí predictions shape: {preds.shape}\")\n",
    "print(f\"üïí Inference time: {(time.time() - start):.1f}s\")\n",
    "\n",
    "# =============================================================\n",
    "# üîπ Format Submission\n",
    "# =============================================================\n",
    "target_cols = [\"Dry_Clover_g\", \"Dry_Dead_g\", \"Dry_Green_g\", \"Dry_Total_g\", \"GDM_g\"]\n",
    "unique_images = test_df[\"image_path\"].unique()\n",
    "\n",
    "if len(unique_images) != preds.shape[0]:\n",
    "    print(f\"‚ö†Ô∏è Adjusting predictions: {preds.shape[0]} rows vs {len(unique_images)} images\")\n",
    "    unique_images = np.resize(unique_images, preds.shape[0])\n",
    "\n",
    "preds_df = pd.DataFrame(preds, columns=target_cols)\n",
    "preds_df[\"image_path\"] = unique_images\n",
    "\n",
    "sub_df = preds_df.melt(\n",
    "    id_vars=\"image_path\", var_name=\"target_name\", value_name=\"target\"\n",
    ")\n",
    "final_sub = (\n",
    "    test_df[[\"sample_id\", \"image_path\", \"target_name\"]]\n",
    "    .merge(sub_df, on=[\"image_path\", \"target_name\"], how=\"left\")\n",
    "    .drop_duplicates(subset=[\"sample_id\"])\n",
    ")\n",
    "\n",
    "# =============================================================\n",
    "# üíæ Save & Report\n",
    "# =============================================================\n",
    "sub_path = OUTPUT_DIR / \"submission.csv\"\n",
    "final_sub[[\"sample_id\", \"target\"]].to_csv(sub_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Saved submission ‚Üí {sub_path.resolve()}\")\n",
    "print(f\"üì¶ Submission shape: {final_sub.shape}\")\n",
    "print(final_sub[\"target\"].describe().to_string())\n",
    "print(\"üèÅ Ready for Kaggle upload.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b35e1ac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
